{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAFE Protocol NLP Experiments \n",
    "\n",
    "\n",
    "This notebook builds on top of the other notebook where I [illustrated the bare SAFE Protocol itself](./safe-illustration.ipynb). \n",
    "\n",
    "This notebook specifically illustrates the application of SAFE to trend detection in document interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trending Mood Detection For Survey Responses\n",
    "\n",
    "As described in the report, users respond to a prompt question like \"*How do you feel today?*\" every day for a month, and an analysis on what responses are trending is carried out every 2 weeks.\n",
    "\n",
    "When submitting a response to this question, the user picks from a pre-defined list of responses that contains strings like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_responses = {\n",
    "    0: \"I'm feeling joyful!\",\n",
    "    1: \"I'm feeling angry\",\n",
    "    2: \"I'm feeling disgusted\",\n",
    "    3: \"I'm feeling fearful\",\n",
    "    4: \"I'm feeling sad...\",\n",
    "    5: \"I'm feeling surprised!\",\n",
    "    6: \"I'm feeling neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These responses are each encoded as numbers, such that what is actually stored locally on user devices as their responses are integers from `0` to `6` corresponding to each of these answers (`0` corresponds to `I'm feeling joyful!\"`, `1` corresponds to `\"I'm feeling angry\"`, and so on...)\n",
    "\n",
    "The target computation of the Bayesian approach to secure trend detection is:\n",
    "\n",
    "$$\n",
    "p(\\text{t} \\space =\\space t|D)= \\frac {p(D|\\text{t} \\space =\\space t|)p(\\text{t} \\space =\\space t|)}{p(D)} \n",
    "$$\n",
    "\n",
    "where $t$ is some term in the keyword set $V$. \n",
    "\n",
    "The **keyword set** $V$ for our experimental setting consists of the responses themselves, since what we what to determine is a probability distribution for the various moods given the overall document set contributed by users.\n",
    "\n",
    "As discussed in their paper, calculating the marginal probablity $p(D)$ would not be privacy-preserving, as this would require access to the dataset $D$. The authors advise avoiding calculating it by treating it as constant. Then, the posterior likelihood $p(\\text{t} \\space =\\space t|D)$ is proportional to ${p(D|\\text{t} \\space =\\space t|)p(\\text{t} \\space =\\space t|)}$. Since we are only trying to find trending keywords (in terms of rankings), we do not need the exact value of the posterior and we can simply consider this equation:\n",
    "\n",
    "$$\n",
    "p(\\text{t} \\space =\\space t|D) \\propto {p(D|\\text{t} \\space =\\space t|)p(\\text{t} \\space =\\space t|)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Defining Our Terms\n",
    "\n",
    "In particular, we want to define the terms on the RHS : the likelihood, $p(D|\\text{t} \\space =\\space t|)$, and the prior $p(\\text{t} \\space =\\space t|)$.\n",
    "\n",
    "##### Defining The Prior\n",
    "\n",
    "For the first run of the protocol, the priors will be defined uniformly-- i.e., $\\frac{1}{d}$ where $d$ is the number of keywords in $V$.\n",
    "\n",
    "For each subsequent run, the priors will be defined according to last round's posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the Likelihood Vector\n",
    "\n",
    "Let's suppose there are $N$ users who respond to the prompt question every day for $y$ days. (We assume that users respond every day with some response)\n",
    "\n",
    "This will mean that each user has a list of $y$ integer responses that represents the option they picked for each day. \n",
    "\n",
    "This will constitute their **document set** $D_{i}$ which we wish remains private, where *each integer* (representing a response) is a **document**.\n",
    "\n",
    "Each document's **primary keyword set** is simply a single-element set with the integer that represents the response they picked. (e.g. the primary keyword set of the document `1` just is `1`).\n",
    "\n",
    "As described in the paper, what ends up being the user's raw feature vector is a vector of likelihoods,\n",
    "\n",
    "$$\n",
    "L_{i}  =(p( D_ {i}  |\\text{t} \\space =\\space t_ {1}  ), \\cdots  ,p(  D_ {i}  |\\text{t} \\space =\\space t_ {d}  ))\n",
    "$$\n",
    "\n",
    "where $d$ is the number of words in the vocabulary $V$, and each $p(D_{i} | \\text{t} \\space =\\space t)$ is calculated by the fraction of the given keyword in the document set of the user.\n",
    "\n",
    "For instance, say that a user has the following document set of just 7 responses (for simplicity):\n",
    "\n",
    "```\n",
    "u_1 = [0, 1, 2, ,3, 1, 4, 5]  \n",
    "```\n",
    "\n",
    "They have 7 documents in total. Their feature vector of likelihoods is\n",
    "\n",
    "```\n",
    "V = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "u_1 = [0, 1, 2, 3, 1, 4, 5]  \n",
    "\n",
    "u_1_feature_vector = [1/7, 2/7, 1/7, 1/7, 1/7,  1/7, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing The Target Function\n",
    "\n",
    "Treating each user's document set $D_i$ as a random variable for the subset of the overall document set $D$ (i.e., the set of all user responses), then we can treat the aggregation of these instances (i.e., the likelihood of the whole document set) can be represented as the mean. (ibid., 5)\n",
    "\n",
    "So, the aggregator wants to compute the following for each keyword.\n",
    "\n",
    "$$\n",
    "p(D | \\text{t} \\space =\\space t_1) = \\sum_{i = 1}^N {p(D|\\text{t} \\space =\\space t_1|)}\n",
    "$$\n",
    "\n",
    "Aggregating the feature vectors for all users can be done securely using the SAFE Protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run : With Raw SAFE version\n",
    "\n",
    "#### Generating The Raw Feature Vectors\n",
    "\n",
    "Let's say there are 10 users who respond to the prompt question every day for 21 days, and as analysts we want to consider what responses are trending given this period.\n",
    "\n",
    "First, let's define our constants and import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from itertools import repeat\n",
    "from collections import Counter\n",
    "from MoodAppUser import MoodAppUser\n",
    "\n",
    "r.seed(123) # for reproducibility \n",
    "\n",
    "# as from above: \n",
    "POSSIBLE_RESPONSES = {\n",
    "    0: \"I'm feeling joyful!\",\n",
    "    1: \"I'm feeling angry\",\n",
    "    2: \"I'm feeling disgusted\",\n",
    "    3: \"I'm feeling fearful\",\n",
    "    4: \"I'm feeling sad...\",\n",
    "    5: \"I'm feeling surprised!\",\n",
    "    6: \"I'm feeling neutral\",\n",
    "}\n",
    "KEYWORDS = [0, 1, 2, 3, 4, 5, 6] # corresponding to our 7-emotion taxonomy\n",
    "\n",
    "NO_OF_DAYS_TRACKED = 21\n",
    "NO_OF_USERS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create the 10 users and give them document sets that consists of a random selection of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_document_sets = [r.choices(KEYWORDS, k = NO_OF_DAYS_TRACKED) for _ in repeat(None,   NO_OF_USERS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Distribution Of Randomly Generated Document Sets\n",
    "\n",
    "**Note**: according to the [documentation](https://docs.python.org/3/library/random.html) on `random.choices()` (which is the method we invoked to form our random document sets)\n",
    "\n",
    ">If neither weights nor cum_weights are specified, selections are made with equal probability. \n",
    "\n",
    "Since we did not specify any arguments for the `weights` nor `cum_weights` parameters, we should except out document sets to have a roughly **flat distribution**.\n",
    "\n",
    "This is just worth bearing in mind when calculate the posterior. It is not a problem since we are merely illustrating the protocol's application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some users and their random document sets:\n",
      "\n",
      "User 0 and their random document set:\n",
      "[0, 0, 2, 0, 6, 0, 3, 2, 5, 1, 2, 2, 1, 0, 3, 0, 4, 0, 2, 3, 6]\n",
      "\n",
      "User 1 and their random document set:\n",
      "[0, 0, 5, 0, 6, 4, 1, 5, 5, 2, 5, 1, 4, 3, 5, 2, 2, 5, 3, 4, 4]\n",
      "\n",
      "User 2 and their random document set:\n",
      "[4, 6, 3, 4, 2, 0, 5, 1, 5, 6, 4, 1, 2, 2, 0, 3, 4, 2, 1, 0, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = [MoodAppUser(i, document_set) for i, document_set in enumerate(random_document_sets)]\n",
    "\n",
    "print(\"Here are some users and their random document sets:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"User {users[i].id} and their random document set:\\n{users[i].document_set}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at their raw feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some users and their raw feature vectors:\n",
      "\n",
      "User 0 and their raw feature vector:\n",
      "[0.3333 0.2381 0.0952 0.1429 0.0476 0.0952 0.0476] of length 7\n",
      "\n",
      "User 1 and their raw feature vector:\n",
      "[0.1429 0.2857 0.0476 0.1905 0.0952 0.1429 0.0952] of length 7\n",
      "\n",
      "User 2 and their raw feature vector:\n",
      "[0.2381 0.0952 0.0952 0.1905 0.1429 0.0952 0.1429] of length 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are some users and their raw feature vectors:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"User {users[i].id} and their raw feature vector:\\n{users[i].feature_vector} of length {len(users[i].feature_vector)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Secure Aggregation of Feature Vectors\n",
    "\n",
    "Now that we have users and their raw feature vectors, let's compute the target function. \n",
    "\n",
    "Since this is the first run of the protocol, we begin with uniform priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_keywords = len(KEYWORDS)\n",
    "priors_for_keywords = np.round([(1 / no_of_keywords) for _ in repeat(None, no_of_keywords)], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate using the raw feature vectors what the target value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Posterior For Initial Round:\n",
      "\n",
      "array([0.26539388, 0.27219592, 0.21093469, 0.25181838, 0.13605509,\n",
      "       0.18372653, 0.10886122])\n"
     ]
    }
   ],
   "source": [
    "target_aggregation = np.sum(list([user.feature_vector for user in users]), axis = 0)\n",
    "\n",
    "target_posterior = np.multiply(target_aggregation, priors_for_keywords)\n",
    "\n",
    "print(\"Target Posterior For Initial Round:\\n\")\n",
    "pprint(target_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the same result running SAFE. Let's see if that happens:\n",
    "\n",
    "#### Share Generation\n",
    "\n",
    "First, let's have each user generate the N shares.\n",
    "\n",
    "Users first want to generate $N-1$ shares to send to other users. \n",
    "\n",
    "They will also generate their $N^{th}$ share (to keep) using these.\n",
    "\n",
    "First, we set the D value for the round, and then users will draw shares from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoodAppUser.set_D_value()\n",
    "\n",
    "for i in range(NO_OF_USERS):\n",
    "    users[i].generate_shares_to_send(NO_OF_USERS, no_of_keywords)\n",
    "    users[i].calculate_Nth_share()\n",
    "\n",
    "# SANITY CHECK\n",
    "assert np.all(np.subtract(users[0].feature_vector, np.sum(list(users[0].shares_to_send.values()), axis = 0))  == users[0].Nth_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Share Distribution\n",
    "\n",
    "Now we wish to distribute the shares among the users.\n",
    "\n",
    "User 0 will send the shares they generated for User 1 and User 2 to each respectively, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(NO_OF_USERS):\n",
    "    other_user_ids = [id for id in range(NO_OF_USERS) if id != user]\n",
    "    for other_user in other_user_ids:\n",
    "        share_to_send = users[user].get_share_for_user(other_user)\n",
    "        users[other_user].receive_share(share_to_send)\n",
    "    # NOTE - as noted below, we can generate the obfuscated vector here if we want to be more efficient.\n",
    "\n",
    "# SANITY CHECK\n",
    "assert np.all(users[0].shares_to_send[1] == users[1].shares_received[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Obfuscated Feature Vector\n",
    "\n",
    "Having distributed the shares, each user can now calculate their obfuscated feature vector.\n",
    "\n",
    "They do this by adding the sum of received shares to their Nth share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in range(NO_OF_USERS):\n",
    "    users[user].generate_obfuscated_feature_v()\n",
    "\n",
    "# NOTE - share distribution and obfuscated vector generation can be done in the same loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secure Aggregation & Posterior Calculation\n",
    "\n",
    "Let's now see if aggregating the obfuscated feature vectors results in the same result as aggregating the raw feature vectors. If so, our protocol has been implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of raw vectors: \n",
      "\n",
      " [1.8572 1.9048 1.4761 1.7622 0.9521 1.2857 0.7618]\n",
      "Sum of masked vectors: \n",
      "\n",
      " [1.8572 1.9048 1.4761 1.7622 0.9521 1.2857 0.7618]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked_vector_result = np.sum(list([user.obfuscated_feature_v for user in users]), axis = 0)\n",
    "\n",
    "# rounding because precision beyond 4.d.p is unlikely to be consequential :\n",
    "target_aggregation = np.round(target_aggregation, 4) \n",
    "masked_vector_result = np.round(masked_vector_result, 4)\n",
    "assert np.array_equal(target_aggregation, masked_vector_result)\n",
    "\n",
    "print(f\"Sum of raw vectors: \\n\\n {target_aggregation}\")\n",
    "print(f\"Sum of masked vectors: \\n\\n {masked_vector_result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! And now we calculate the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Posterior value: \n",
      "\n",
      " [0.2654 0.2722 0.2109 0.2518 0.1361 0.1837 0.1089]\n",
      "Masked Posterior value: \n",
      "\n",
      " [0.2654 0.2722 0.2109 0.2518 0.1361 0.1837 0.1089]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked_posterior = np.multiply(masked_vector_result, priors_for_keywords)\n",
    "\n",
    "# rounding because precision beyond 4.d.p is unlikely to be consequential :\n",
    "target_posterior = np.round(target_posterior, 4) \n",
    "masked_posterior = np.round(masked_posterior, 4)\n",
    "assert np.array_equal(target_posterior, masked_posterior)\n",
    "\n",
    "print(f\"Raw Posterior value: \\n\\n {target_posterior}\")\n",
    "print(f\"Masked Posterior value: \\n\\n {masked_posterior}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Trends In Interactions Over Some Static List Of Resources\n",
    "\n",
    "A neat feature of the above protocol is that it can not only be used to detect trends in user responses to the prompt question, but also to detect trends for **interactions with a static list of resources**.\n",
    "\n",
    "We simply map the list of resources accessed by a user to numbers just as we did with the possible questions. \n",
    "\n",
    "For instance, say we had a static list of 3 resources, 3 active users and collected their inputs for 5 days. Then for a given user we'd have something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as from above: \n",
    "RESOURCES = [0, 1, 2]\n",
    "\n",
    "example_document_set = {\n",
    "    0: [0, 1, 2],\n",
    "    1: [0, 1, 2],\n",
    "    2: [0, 1],\n",
    "    3: [],\n",
    "    4: [2]\n",
    "}\n",
    "\n",
    "KEYWORDS = [0, 1, 2] # corresponding to our keywords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protocol can then be run in a similar fashion, so long as we **flatten the document set into a list representation**. So, we'd be able to compute a probability distribution (posterior) for which resources are 'trending' given the overall document set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of Approach:\n",
    "\n",
    "One issue that should be addressed is how to account for the fact that users may not use the app every day. How should we encode and handle non-responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as from above: \n",
    "POSSIBLE_RESPONSES = {\n",
    "    0: \"I'm feeling joyful!\",\n",
    "    1: \"I'm feeling angry\",\n",
    "    2: \"I'm feeling disgusted\",\n",
    "    3: \"I'm feeling fearful\",\n",
    "    4: \"I'm feeling sad...\",\n",
    "    5: \"I'm feeling surprised!\",\n",
    "    6: \"I'm feeling neutral\",\n",
    "    -1: None # no response for that day.\n",
    "}\n",
    "\n",
    "u_1 = MoodAppUser(0, [0, 1, 2, 3, 1, 4, 5, -1]) # 8 days of collected dataw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just include `-1` as a keyword in the response. However, as with any dataset with missing values, the utility of our analysis can be adversely impacted if there are too many non-responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
